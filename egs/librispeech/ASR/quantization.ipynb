{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7c0308-a83e-4e9a-a1cb-00784ca936db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu129 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n",
      "/home/shahn/miniconda3/envs/torch2.9/lib/python3.12/site-packages/torchao/quantization/pt2e/utils.py:145: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n",
      "/home/shahn/miniconda3/envs/torch2.9/lib/python3.12/site-packages/torchao/quantization/pt2e/observer.py:1360: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class M(torch.nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      self.linear = torch.nn.Linear(5, 10)\n",
    "\n",
    "   def forward(self, x):\n",
    "      return self.linear(x)\n",
    "\n",
    "\n",
    "example_inputs = (torch.randn(1, 5),)\n",
    "m = M().eval()\n",
    "\n",
    "# Step 1. program capture\n",
    "# This is available for pytorch 2.6+, for more details on lower pytorch versions\n",
    "# please check `Export the model with torch.export` section\n",
    "m = torch.export.export(m, example_inputs).module()\n",
    "# we get a model with aten ops\n",
    "\n",
    "\n",
    "# Step 2. quantization\n",
    "from torchao.quantization.pt2e.quantize_pt2e import (\n",
    "  prepare_pt2e,\n",
    "  convert_pt2e,\n",
    ")\n",
    "\n",
    "# install executorch: `pip install executorch`\n",
    "from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import (\n",
    "  get_symmetric_quantization_config,\n",
    "  XNNPACKQuantizer,\n",
    ")\n",
    "# backend developer will write their own Quantizer and expose methods to allow\n",
    "# users to express how they\n",
    "# want the model to be quantized\n",
    "quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\n",
    "m = prepare_pt2e(m, quantizer)\n",
    "\n",
    "# calibration omitted\n",
    "\n",
    "m = convert_pt2e(m)\n",
    "# we have a model with aten ops doing integer computations when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "076f6b58-34ce-4e8d-8734-5379821def94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchao.quantization' has no attribute 'QuantStub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# torch.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# torch.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"Insert stubs\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m m = nn.Sequential(\u001b[43mtorchao\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQuantStub\u001b[49m(), \n\u001b[32m     26\u001b[39m                   *m, \n\u001b[32m     27\u001b[39m                   torchao.quantization.DeQuantStub())\n\u001b[32m     29\u001b[39m \u001b[33;03m\"\"\"Prepare\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m m.qconfig = torchao.quantization.get_default_qconfig(backend)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torchao.quantization' has no attribute 'QuantStub'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
    "\n",
    "model = nn.Sequential(\n",
    "     nn.Conv2d(2,64,3),\n",
    "     nn.ReLU(),\n",
    "     nn.Conv2d(64, 128, 3),\n",
    "     nn.ReLU()\n",
    ")\n",
    "\n",
    "## EAGER MODE\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "\"\"\"Fuse\n",
    "- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules\n",
    "\"\"\"\n",
    "# torch.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\n",
    "# torch.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\n",
    "\n",
    "\"\"\"Insert stubs\"\"\"\n",
    "m = nn.Sequential(torchao.quantization.QuantStub(), \n",
    "                  *m, \n",
    "                  torchao.quantization.DeQuantStub())\n",
    "\n",
    "\"\"\"Prepare\"\"\"\n",
    "m.qconfig = torchao.quantization.get_default_qconfig(backend)\n",
    "torchao.quantization.prepare(m, inplace=True)\n",
    "\n",
    "\"\"\"Calibrate\n",
    "- This example uses random data for convenience. Use representative (validation) data instead.\n",
    "\"\"\"\n",
    "with torch.inference_mode():\n",
    "  for _ in range(10):\n",
    "    x = torch.rand(1,2, 28, 28)\n",
    "    m(x)\n",
    "    \n",
    "\"\"\"Convert\"\"\"\n",
    "torchao.quantization.convert(m, inplace=True)\n",
    "\n",
    "\"\"\"Check\"\"\"\n",
    "print(m[1].weight().element_size()) # 1 byte instead of 4 bytes for FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37720777-47bb-4de6-92fc-343d7585c3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Conv2d.weight of QuantizedConv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.018997007980942726, zero_point=71)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa2893-45d5-4c5b-8fa5-893024860752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu129 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "class ToyLinearModel(torch.nn.Module):\n",
    "    def __init__(self, m=64, n=32, k=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(m, k, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(k, n, bias=True)\n",
    "\n",
    "    def example_inputs(self, batch_size=1, dtype=torch.float32, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.randn(\n",
    "                batch_size, self.linear1.in_features, dtype=dtype, device=device\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "m = ToyLinearModel(4, 4, 4).eval().to(dtype).to(device)\n",
    "m = torch.compile(m, mode=\"max-autotune\")\n",
    "\n",
    "from torchao.quantization.granularity import PerAxis, PerTensor\n",
    "from torchao.quantization.observer import AffineQuantizedMinMaxObserver\n",
    "from torchao.quantization.quant_primitives import MappingType, ZeroPointDomain\n",
    "\n",
    "# per tensor input activation asymmetric quantization\n",
    "act_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.SYMMETRIC,\n",
    "    torch.int8,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    "    zero_point_domain=ZeroPointDomain.NONE\n",
    ")\n",
    "\n",
    "# per channel weight asymmetric quantization\n",
    "weight_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.SYMMETRIC,\n",
    "    torch.int8,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    "    zero_point_domain=ZeroPointDomain.NONE\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ObservedLinear(torch.nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.act_obs = act_obs\n",
    "        self.weight_obs = weight_obs\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        observed_input = self.act_obs(input)\n",
    "        observed_weight = self.weight_obs(self.weight)\n",
    "        return F.linear(observed_input, observed_weight, self.bias)\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, float_linear, act_obs, weight_obs):\n",
    "        observed_linear = cls(\n",
    "            float_linear.in_features,\n",
    "            float_linear.out_features,\n",
    "            act_obs,\n",
    "            weight_obs,\n",
    "            True,\n",
    "            device=float_linear.weight.device,\n",
    "            dtype=float_linear.weight.dtype,\n",
    "        )\n",
    "        observed_linear.weight = float_linear.weight\n",
    "        observed_linear.bias = float_linear.bias\n",
    "        return observed_linear\n",
    "\n",
    "\n",
    "from torchao.quantization.quant_api import (\n",
    "    _replace_with_custom_fn_if_matches_filter,\n",
    ")\n",
    "\n",
    "def insert_observers_(model, act_obs, weight_obs):\n",
    "    _is_linear = lambda m, fqn: isinstance(m, torch.nn.Linear)\n",
    "\n",
    "    def replacement_fn(m):\n",
    "        copied_act_obs = copy.deepcopy(act_obs)\n",
    "        copied_weight_obs = copy.deepcopy(weight_obs)\n",
    "        return ObservedLinear.from_float(m, copied_act_obs, copied_weight_obs)\n",
    "\n",
    "    _replace_with_custom_fn_if_matches_filter(model, replacement_fn, _is_linear)\n",
    "\n",
    "insert_observers_(m, act_obs, weight_obs)\n",
    "\n",
    "for _ in range(10):\n",
    "    example_inputs = m.example_inputs(dtype=dtype, device=device)\n",
    "    m(*example_inputs)\n",
    "\n",
    "from torchao.dtypes import to_affine_quantized_intx_static\n",
    "\n",
    "class QuantizedLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        weight: torch.Tensor,\n",
    "        bias: torch.Tensor,\n",
    "        target_dtype: torch.dtype,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act_scale, self.act_zero_point = act_obs.calculate_qparams()\n",
    "        weight_scale, weight_zero_point = weight_obs.calculate_qparams()\n",
    "        assert weight.dim() == 2\n",
    "        block_size = (weight.shape[0], weight.shape[1])\n",
    "        self.target_dtype = target_dtype\n",
    "        self.bias = bias\n",
    "        self.qweight = to_affine_quantized_intx_static(\n",
    "            weight, weight_scale, weight_zero_point, block_size, self.target_dtype, zero_point_domain=ZeroPointDomain.NONE\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        block_size = input.shape\n",
    "        qinput = to_affine_quantized_intx_static(\n",
    "            input,\n",
    "            self.act_scale,\n",
    "            self.act_zero_point,\n",
    "            block_size,\n",
    "            self.target_dtype,\n",
    "            zero_point_domain=ZeroPointDomain.NONE,\n",
    "        )\n",
    "        return F.linear(qinput, self.qweight, self.bias)\n",
    "\n",
    "    def forward_int8(self, input: torch.Tensor):\n",
    "        block_size = input.shape\n",
    "        qinput = to_affine_quantized_intx_static(\n",
    "            input,\n",
    "            self.act_scale,\n",
    "            self.act_zero_point,\n",
    "            block_size,\n",
    "            self.target_dtype,\n",
    "            zero_point_domain=ZeroPointDomain.NONE,\n",
    "        )\n",
    "        int8_i, scale_i, _ = qinput.tensor_impl.get_plain()\n",
    "        int8_w, scale_w, _ = self.qweight.tensor_impl.get_plain()\n",
    "        print(F.linear(int8_i.to(torch.int32), int8_w.to(torch.int32)))\n",
    "        return F.linear(int8_i.to(torch.int32), int8_w.to(torch.int32)).to(torch.float32) * (scale_i*scale_w) + self.bias\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, observed_linear, target_dtype):\n",
    "        quantized_linear = cls(\n",
    "            observed_linear.in_features,\n",
    "            observed_linear.out_features,\n",
    "            observed_linear.act_obs,\n",
    "            observed_linear.weight_obs,\n",
    "            observed_linear.weight,\n",
    "            observed_linear.bias,\n",
    "            target_dtype,\n",
    "        )\n",
    "        return quantized_linear\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torchao.core.config import AOBaseConfig\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.transform_module import (\n",
    "    register_quantize_module_handler,\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class StaticQuantConfig(AOBaseConfig):\n",
    "    target_dtype: torch.dtype\n",
    "\n",
    "@register_quantize_module_handler(StaticQuantConfig)\n",
    "def _apply_static_quant(\n",
    "    module: torch.nn.Module,\n",
    "    config: StaticQuantConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    Define a transformation associated with `StaticQuantConfig`.\n",
    "    This is called by `quantize_`, not by the user directly.\n",
    "    \"\"\"\n",
    "    return QuantizedLinear.from_observed(module, config.target_dtype)\n",
    "\n",
    "# filter function to identify which modules to swap\n",
    "is_observed_linear = lambda m, fqn: isinstance(m, ObservedLinear)\n",
    "\n",
    "# perform static quantization\n",
    "quantize_(m, StaticQuantConfig(torch.int8), is_observed_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11795a06-ab6b-4371-abf4-04502419a81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-13782,   4494,   8964,  12220],\n",
      "         [ 17631,   6600,  -1550,  -9179],\n",
      "         [ -3563,  -2746,  -2081,   -352],\n",
      "         [ -6578,    680,    790,   3260],\n",
      "         [ -2544,  -1523,  -5267,  -2687],\n",
      "         [     3,  -3224, -12286,  -6949],\n",
      "         [-14733, -11205,   6339,  13776],\n",
      "         [ -7560,  -1663,    130,  -1248],\n",
      "         [ -8867,  11408,  -6545,  -2031],\n",
      "         [  3933,  11864, -11642, -14015],\n",
      "         [ 11648,  -2793,  -6540, -11410],\n",
      "         [ -8015,  -3694,   -529,  -2376],\n",
      "         [  1789,    606, -11456, -10634],\n",
      "         [ -6238,  -7694, -16127,  -7283],\n",
      "         [  5782,   9062,  -1450,  -5505],\n",
      "         [  7691,    180,   5236,   4404],\n",
      "         [ 12443,   3018,  -6990,  -9822],\n",
      "         [-11288,  -4266,  -6744,   -902],\n",
      "         [-11572,   3514,   6744,   9116],\n",
      "         [  1184,  -4354,   2481,   2398],\n",
      "         [ -8030,    977,   2732,   2441],\n",
      "         [-10714,  -3620,  12765,  17869],\n",
      "         [  7956, -13467,  -6927,  -6749],\n",
      "         [ -1595,   -961,   2433,   4962],\n",
      "         [ -3471,   -537,   2057,  -1372],\n",
      "         [  3167,  -3245,    892,  -3636],\n",
      "         [ -1602,  -4660, -14967, -14413],\n",
      "         [  3946,   4881, -11488, -11070],\n",
      "         [-12814,   4374,  -7857,  -2378],\n",
      "         [ -8352,   4007,   3703,   7112],\n",
      "         [ -6846,  -1798,    181,    520],\n",
      "         [  8093,    750,   2414,  -2987],\n",
      "         [  6397,  -4378,  -1136,  -2461],\n",
      "         [ -8216,   4474,   5068,   5738],\n",
      "         [ -8067,   5008,   3405,   6421],\n",
      "         [ -2368,  -2743,  -3720,  -2221],\n",
      "         [ -1141,     86,  -4922,  -3083],\n",
      "         [ -9331,  -6941,  12794,  14282],\n",
      "         [ -5663,    715,   3793,   1340],\n",
      "         [ 16942,  -1969,  -1207,  -5634],\n",
      "         [  5004,   7353,   8471,   3450],\n",
      "         [ -2912,  -9609, -10277,  -7556],\n",
      "         [  9384,   5361,  -9732, -11514],\n",
      "         [ 10206,  12199, -14327, -17281],\n",
      "         [-10872,   1426,  -7799,  -2421],\n",
      "         [-16578,    403,  -4550,   2292],\n",
      "         [  2359,  -7901,   2936,   4522],\n",
      "         [  4428,  -4098,   7601,   7975],\n",
      "         [  3131,  -6332,  -3098,  -1889],\n",
      "         [ -6192,   -274,   -861,    971],\n",
      "         [  8133,  13634,   3400,   1889],\n",
      "         [ -3069,   3517,   7750,   6355],\n",
      "         [  8035,    -95,  10785,   5370],\n",
      "         [ -3584,  -1499,  -3061,  -4374],\n",
      "         [  -680,  -2718,  -6354,  -2021],\n",
      "         [ -1993,  -7257,     63,   2993],\n",
      "         [  2841,   4572,  -4097,  -9220],\n",
      "         [  -106,  -1933,  -1674,  -3907],\n",
      "         [ 11684,   7516,  -3827, -10094],\n",
      "         [ 13409,     93,   2985,  -2887],\n",
      "         [ -7304,  -1034,   2363,   1205],\n",
      "         [  -195,  -2485,  -4158,  -1972],\n",
      "         [ 13796,  -3581,  -3393,  -7783],\n",
      "         [ -8519,  -5261,  -3463,   -167],\n",
      "         [ -4250,   5219,  -2568,  -4588],\n",
      "         [  7949,  -7162,   1073,  -2646],\n",
      "         [  3206,   7205,  -5931,  -5899],\n",
      "         [  7076,  13911,  13772,   8381],\n",
      "         [  1870,  -3070,   3799,   5686],\n",
      "         [ -2731,   5097,   5164,   5035],\n",
      "         [  1417,   2118,  -5871,  -8676],\n",
      "         [  5283,   6610,  -4999,  -8129],\n",
      "         [  4308,    842, -17318, -18074],\n",
      "         [  4503,  -5056,   3587,   2102],\n",
      "         [  8996,  10929,  -7432, -13591],\n",
      "         [ -7321,  -9348,  -5898,  -5121],\n",
      "         [  7639,   9799,   2005,    735],\n",
      "         [ 10725,   2930,  -4289, -11245],\n",
      "         [  5527,  -9499,  -1449,   -566],\n",
      "         [   190,  -1786,   -168,    593],\n",
      "         [ 12470,  -7963,  -5502, -10893],\n",
      "         [   201,   3332,   3311,  -2740],\n",
      "         [  3428,  -6461,  -5647,  -2424],\n",
      "         [ -5992,   3148,   4769,   3762],\n",
      "         [  8188, -12608,   -259,    843],\n",
      "         [-10846,  12828,  11374,  10616],\n",
      "         [  2984,   5800,   3345,  -2573],\n",
      "         [ -7025,   3947,  11703,  14327],\n",
      "         [  2586,  -2995,  11168,  13556],\n",
      "         [ -5234,  11560,   8446,   8630],\n",
      "         [ -1923,  -5288,   2977,   4267],\n",
      "         [  4214,   9283,  -1912,  -6865],\n",
      "         [ -2043,  -1143,  -6418,  -3220],\n",
      "         [ -7093, -11281,   -315,   5275],\n",
      "         [  9305,  -6409,  -2790,  -6667],\n",
      "         [   980,  -2013,   6537,   4812],\n",
      "         [ -2155,  15363,   9602,   1908],\n",
      "         [ -2675, -16989,  -1304,   4416],\n",
      "         [ -4808,   3896,   7632,   9270],\n",
      "         [ -4728,   3127, -11097, -12064]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 100, 4, dtype=dtype, device=device)\n",
    "y = m.linear1.forward_int8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f5f49f7-e5ea-4232-9bd2-65e53099ed94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = m.linear1.forward(x)\n",
    "torch.allclose(y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1963bdd1-2103-4c0c-854b-2cbc762375fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-128,   -1,   -1,    0,    0,    0,    1,    1,    2,    2,   44,  127],\n",
       "        dtype=torch.int8),\n",
       " tensor([0.0228]),\n",
       " None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-10, -0.0253, -0.0127, -0.0064, 0.0, 0.0064, 0.0127, 0.0253, 0.0380, 0.0506, 1.0, 10])\n",
    "to_affine_quantized_intx_static(\n",
    "    x,\n",
    "    m.linear1.act_scale,\n",
    "    m.linear1.act_zero_point,\n",
    "    x.shape,\n",
    "    m.linear1.target_dtype,\n",
    "    zero_point_domain=ZeroPointDomain.NONE,\n",
    ").tensor_impl.get_plain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4903b7-f1f5-487f-9bef-6dc2e34d7325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-439.,   -1.,   -1.,   -0.,    0.,    0.,    1.,    1.,    2.,    2.,\n",
       "          44.,  439.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(x / m.linear1.act_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2287c595-7f0e-4c04-b16f-d65003b5bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bec87a7b-7bf5-4a1d-9be5-3b2489e04332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "dtype_q = torch.int8\n",
    "\n",
    "class ToyModel(torch.nn.Module):\n",
    "    def __init__(self, ci=64, ch=32, co=64, k=3):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(ci, ch, k, padding=(k-1)//2, bias=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(ch, co, bias=True)\n",
    "\n",
    "    def set_int8_mode(self):\n",
    "        self.conv.set_int8_mode()\n",
    "        self.linear.set_int8_mode()\n",
    "\n",
    "    def set_quantized_mode(self):\n",
    "        self.conv.set_quantized_mode()\n",
    "        self.linear.set_quantized_mode()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: [B, Ci, T]\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = ToyModel(4, 4, 4).eval().to(dtype).to(device)\n",
    "m = copy.deepcopy(model)\n",
    "# m = torch.compile(m, mode=\"max-autotune\")\n",
    "\n",
    "from torchao.quantization.granularity import PerTensor\n",
    "from torchao.quantization.observer import AffineQuantizedMinMaxObserver\n",
    "from torchao.quantization.quant_primitives import MappingType, ZeroPointDomain\n",
    "from torchao.dtypes import to_affine_quantized_intx_static\n",
    "\n",
    "# per tensor input activation asymmetric quantization\n",
    "act_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.SYMMETRIC,\n",
    "    dtype_q,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    "    zero_point_domain=ZeroPointDomain.NONE\n",
    ")\n",
    "\n",
    "# per channel weight asymmetric quantization\n",
    "weight_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.SYMMETRIC,\n",
    "    dtype_q,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    "    zero_point_domain=ZeroPointDomain.NONE\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QModule(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        module: torch.nn.Module,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.act_obs = act_obs\n",
    "        self.weight_obs = weight_obs\n",
    "        self.mode = \"observe\"\n",
    "        self.act_scale = 0.0\n",
    "        self.act_zero_point = None\n",
    "        self.target_dtype = dtype_q\n",
    "\n",
    "    def set_quantized_mode(self):\n",
    "        if self.mode == \"int8\":\n",
    "            self.mode = \"quantized\"\n",
    "            return\n",
    "        elif self.mode == \"quantized\":\n",
    "            return\n",
    "        self.mode = \"quantized\"\n",
    "        self.act_scale, self.act_zero_point = self.act_obs.calculate_qparams()\n",
    "        weight_scale, weight_zero_point = self.weight_obs.calculate_qparams()\n",
    "        self.weight = to_affine_quantized_intx_static(\n",
    "            self.module.weight,\n",
    "            weight_scale,\n",
    "            weight_zero_point,\n",
    "            self.module.weight.shape,\n",
    "            self.target_dtype,\n",
    "            zero_point_domain=ZeroPointDomain.NONE\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(self.module.bias.data.clone())\n",
    "        # delattr(self.module, \"weight\")\n",
    "        # delattr(self.module, \"bias\")\n",
    "\n",
    "    def set_int8_mode(self):\n",
    "        assert self.mode == \"quantized\"\n",
    "        self.mode = \"int8\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.mode == \"int8\":\n",
    "            qinput = to_affine_quantized_intx_static(\n",
    "                x,\n",
    "                self.act_scale,\n",
    "                self.act_zero_point,\n",
    "                x.shape,\n",
    "                self.target_dtype,\n",
    "                zero_point_domain=ZeroPointDomain.NONE,\n",
    "            )\n",
    "            int8_i, scale_i, _ = qinput.tensor_impl.get_plain()\n",
    "            int8_w, scale_w, _ = self.weight.tensor_impl.get_plain()\n",
    "            return self._forward_int8(int8_i.to(torch.int32), int8_w.to(torch.int32), scale_i * scale_w)\n",
    "        if self.mode == \"observe\":\n",
    "            x = self.act_obs(x)\n",
    "            w = self.weight_obs(self.module.weight)\n",
    "            b = self.module.bias\n",
    "        elif self.mode == \"quantized\":\n",
    "            x = to_affine_quantized_intx_static(\n",
    "                x,\n",
    "                self.act_scale,\n",
    "                self.act_zero_point,\n",
    "                x.shape,\n",
    "                self.target_dtype,\n",
    "                zero_point_domain=ZeroPointDomain.NONE,\n",
    "            )\n",
    "            int8_i, scale_i, _ = x.tensor_impl.get_plain()\n",
    "            int8_w, scale_w, _ = self.weight.tensor_impl.get_plain()\n",
    "            x = int8_i.to(torch.float32) * scale_i\n",
    "            w = int8_w.to(torch.float32) * scale_w\n",
    "            b = self.bias\n",
    "        return self.forward_module(x, w, b)\n",
    "\n",
    "    def forward_module(self, x, w):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class QLinear(QModule):\n",
    "    def forward_module(self, x, w, b):\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "    def _forward_int8(self, x: torch.Tensor, w: torch.Tensor, scale: float):\n",
    "        return F.linear(x, w).to(torch.float32) * scale + self.bias\n",
    "\n",
    "\n",
    "class QConv1d(QModule):\n",
    "    def forward_module(self, x, w, b):\n",
    "        m = self.module\n",
    "        return F.conv1d(x, w, b, stride=m.stride, padding=m.padding, groups=m.groups, dilation=m.dilation)\n",
    "\n",
    "    def _forward_int8(self, x: torch.Tensor, w: torch.Tensor, scale: float):\n",
    "        m = self.module\n",
    "        return F.conv1d(\n",
    "            x, w, stride=m.stride, padding=m.padding, groups=m.groups, dilation=m.dilation\n",
    "        ).to(torch.float32) * scale + self.bias.unsqueeze(1)\n",
    "\n",
    "\n",
    "from torchao.quantization.quant_api import (\n",
    "    _replace_with_custom_fn_if_matches_filter,\n",
    ")\n",
    "\n",
    "def insert_observers_(model, act_obs, weight_obs):\n",
    "    _is_linear = lambda m, fqn: isinstance(m, torch.nn.Linear)\n",
    "    _is_conv = lambda m, fqn: isinstance(m, torch.nn.Conv1d)\n",
    "\n",
    "    def replacement_fn_linear(m):\n",
    "        copied_act_obs = copy.deepcopy(act_obs)\n",
    "        copied_weight_obs = copy.deepcopy(weight_obs)\n",
    "        return QLinear(m, copied_act_obs, copied_weight_obs)\n",
    "\n",
    "    def replacement_fn_conv(m):\n",
    "        copied_act_obs = copy.deepcopy(act_obs)\n",
    "        copied_weight_obs = copy.deepcopy(weight_obs)\n",
    "        return QConv1d(m, copied_act_obs, copied_weight_obs)\n",
    "\n",
    "    _replace_with_custom_fn_if_matches_filter(model, replacement_fn_linear, _is_linear)\n",
    "    _replace_with_custom_fn_if_matches_filter(model, replacement_fn_conv, _is_conv)\n",
    "\n",
    "insert_observers_(m, act_obs, weight_obs)\n",
    "\n",
    "for _ in range(10):\n",
    "    example_inputs = torch.randn(10, 4, 11, dtype=dtype, device=device)\n",
    "    m(example_inputs)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torchao.core.config import AOBaseConfig\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.transform_module import (\n",
    "    register_quantize_module_handler,\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class StaticQuantConfig(AOBaseConfig):\n",
    "    target_dtype: torch.dtype\n",
    "\n",
    "@register_quantize_module_handler(StaticQuantConfig)\n",
    "def _apply_static_quant(\n",
    "    module: torch.nn.Module,\n",
    "    config: StaticQuantConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    Define a transformation associated with `StaticQuantConfig`.\n",
    "    This is called by `quantize_`, not by the user directly.\n",
    "    \"\"\"\n",
    "    return module.set_quantized_mode()\n",
    "\n",
    "# filter function to identify which modules to swap\n",
    "is_observed = lambda m, fqn: isinstance(m, QModule)\n",
    "\n",
    "# perform static quantization\n",
    "quantize_(m, StaticQuantConfig(dtype_q), is_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb801dd2-1e1e-487c-8850-71ff8be304f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5158, -0.5071, -0.3130,  0.5694],\n",
      "         [-0.4772, -0.0242, -0.2216,  0.5604],\n",
      "         [-0.4678,  0.0796, -0.1654,  0.2828],\n",
      "         [-0.5277, -0.0567, -0.1809,  0.3696],\n",
      "         [-0.4450, -0.2705, -0.2657,  0.4772],\n",
      "         [-0.4696, -0.3269, -0.2753,  0.4750],\n",
      "         [-0.6127, -0.1233, -0.1858,  0.5636],\n",
      "         [-0.5405,  0.0497, -0.1543,  0.3666],\n",
      "         [-0.5064,  0.0536, -0.1598,  0.3175],\n",
      "         [-0.4504, -0.1618, -0.2346,  0.3581]]], grad_fn=<AddBackward0>) tensor([[[-0.5223, -0.4976, -0.3084,  0.5709],\n",
      "         [-0.4771, -0.0275, -0.2213,  0.5595],\n",
      "         [-0.4680,  0.0768, -0.1654,  0.2805],\n",
      "         [-0.5270, -0.0602, -0.1829,  0.3697],\n",
      "         [-0.4441, -0.2729, -0.2650,  0.4781],\n",
      "         [-0.4725, -0.3253, -0.2732,  0.4767],\n",
      "         [-0.6136, -0.1215, -0.1863,  0.5613],\n",
      "         [-0.5404,  0.0496, -0.1551,  0.3646],\n",
      "         [-0.5069,  0.0556, -0.1601,  0.3194],\n",
      "         [-0.4508, -0.1663, -0.2352,  0.3616]]], grad_fn=<AddBackward0>) tensor([[[-0.5223, -0.4976, -0.3084,  0.5709],\n",
      "         [-0.4771, -0.0275, -0.2213,  0.5595],\n",
      "         [-0.4680,  0.0768, -0.1654,  0.2805],\n",
      "         [-0.5270, -0.0602, -0.1829,  0.3697],\n",
      "         [-0.4441, -0.2729, -0.2650,  0.4781],\n",
      "         [-0.4725, -0.3253, -0.2732,  0.4767],\n",
      "         [-0.6136, -0.1215, -0.1863,  0.5613],\n",
      "         [-0.5404,  0.0496, -0.1551,  0.3646],\n",
      "         [-0.5069,  0.0556, -0.1601,  0.3194],\n",
      "         [-0.4508, -0.1663, -0.2352,  0.3616]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 4, 10, dtype=dtype, device=device)\n",
    "y_orig = model(x)\n",
    "m.set_quantized_mode()\n",
    "y = m(x)\n",
    "m.set_int8_mode()\n",
    "z = m.forward(x)\n",
    "print(y_orig, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9be7553b-c71d-4c6d-8f6a-d0b8abddec1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "\"normal_kernel_cpu\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m F.conv1d(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m, torch.randn(\u001b[32m4\u001b[39m,\u001b[32m4\u001b[39m,\u001b[32m3\u001b[39m, dtype=torch.int32), padding=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: \"normal_kernel_cpu\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "F.conv1d(torch.randn(1, 4, 100, dtype=torch.int32), torch.randn(4,4,3, dtype=torch.int32), padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57085aa0-85d7-4d49-94f8-dc540f0c8b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
