model="ema_limit_q"
exp="en/w4_lr5e-3_linlr1e-4"

CUDA_VISIBLE_DEVICES=4,5,6,7 ${model}/train.py \
    --world-size 4 \
    --num-epochs 200 \
    --start-epoch 198 \
    --exp-dir exp/$exp \
    --full-libri 1 \
    --max-duration 2400 \
    --master-port 54324 \
    --use-fp16 False \
    --channels 256 \
    --channels-expansion 1024 \
    --dilations-version 11 \
    --kernel-size 8 \
    --encoder-activation ReLU \
    --encoder-se-activation ReLU \
    --skip residual-zeroinit \
    --se-gate tanh \
    --ema-gamma 0.97 \
    --chunksize 8 \
    --encoder-dim 512 \
    --decoder-dim 256 \
    --joiner-dim 256 \
    --encoder-dropout 0.075 \
    --quantizer-gamma 0.95 \
    --eps 1.0e-5 \
    --n-bits-weight 4 \
    --weight-limit 0.3 \
    --data-libri-train True \
    --data-libri-dev-clean True \
    --data-libri-dev-other True \
    --data-ksponspeech-train False \
    --data-ksponspeech-dev False \
    --data-zeroth-train False \
    --data-zeroth-test False \
    --data-command-nor-train False \
    --data-freetalk-nor-train False \
    --on-the-fly-feats False \
    --bpe-model data/en/lang_bpe_500/bpe.model \
    --manifest-dir data/en/fbank \
    --cutset-text text \
    --num-workers 2 \
    --simple-loss-scale 0.5 \
    --optimizer-name Eve \
    --scheduler-name LinearWarmupLR \
    --initial-lr 5e-3 \
    --lr-warmup-iterations 0 \
    --lr-gamma 0.98 \
    --lr-eta-min 1.0e-4 \
    --weight-decay 0.001 \
    --min-utt-duration 1.0 \
    --max-utt-duration 20.0
