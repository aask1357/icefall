model="ema_q_lss"
exp="en/a8w8_lss_glr0.1_jsilu_dim512_bpemax4"
CUDA_VISIBLE_DEVICES=0,1,2,3,4 ${model}/train.py \
    --world-size 4 \
    --num-epochs 200 \
    --start-epoch 1 \
    --exp-dir exp/$exp \
    --full-libri 1 \
    --max-duration 2400 \
    --use-fp16 True \
    --encoder-norm SyncBatchNorm \
    --channels 256 \
    --channels-expansion 1024 \
    --dilations-version 11 \
    --kernel-size 8 \
    --encoder-activation ReLU \
    --encoder-se-activation ReLU \
    --skip residual-zeroinit \
    --se-gate tanh \
    --ema-gamma 0.97 \
    --chunksize 8 \
    --encoder-dropout 0.075 \
    --decoder-dim 512 \
    --joiner-dim 512 \
    --joiner-activation SiLU \
    --eps 1.0e-2 \
    --n-bits-act 8 \
    --n-bits-weight 8 \
    --weight-quantizer-mode scale \
    --quantizer-gamma-lr-ratio 0.1 \
    --bpe-model data/en/bpe_max4/bpe.model \
    --manifest-dir data/en/fbank \
    --cutset-text text \
    --num-workers 2 \
    --simple-loss-scale 0.5 \
    --optimizer-name Eve \
    --weight-decay 0.001 \
    --scheduler-name CosineWarmupLR \
    --initial-lr 5e-3 \
    --lr-warmup-iterations 0 \
    --lr-eta-min 1.0e-4 \
    --min-utt-duration 1.0 \
    --max-utt-duration 20.0
